As a Data Engineer, you are tasked with designing a cloud-based ETL/ELT data pipeline for one
of our company’s divisions. The business needs to have source data extracted from their legacy
database, transformed, and loaded to a cloud database for downstream analytics and reporting.
They need both the raw source data and the transformed output data to be archived in cloud
storage and partitioned by export date. You are also provided with the following information:
● Source data consists of 20-25 different payloads, all of which is stored in CSV format in
the source system.
● Data should be extracted from the source system overnight each night as part of a batch
process
● Source payloads will have to be filtered, joined together, and mapped to new output
payloads with minor data transformations
● The aggregated data should live in a data warehouse in GCP.

Task
1. Briefly describe what GCP services you would rely on to design this pipeline from
beginning to end.
a. This exercise does not require you to write any code.
b. If you think you need additional information that wasn’t called out in the problem
background, be creative and fill in the blanks as you see fit.


For this problem I will use spark, gcs ,airflow,dataproc and bigquery

Bronze Layer
1:setup jobs to load raw data into gcs with partition on export_date

Silver layer
2: Create jobs in spark and run using dataproc to transform and join operation on data and save this to gcs with
partition on export_date

Gold Layer

3: Create another spark job to store aggregated data  in gold layer with partition on export data and schedule job to
append this data in Bigquery datasets

Optimization applied on jobs:

1: Scale Dataproc cluster dynamically based on the load
2: Created table clustering on the basis of downstream user use case also create external table for less frequent
accessed data
3: Apply optimization on spark jobs like
   - repartition and coalesce,
   - use cache and persists,
   - join optimization like broadcast and
sort merge join,
   - skewed data handling,
   - If needed will tune memory parameters of spark
   - Use bucketing from bronze layer for avoid exchange in join operation
   - Use parquet file format
4: I use medallion architecture because of incremental etl ,easy to implements and scalable



